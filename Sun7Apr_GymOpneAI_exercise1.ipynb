{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('FrozenLake-v0')\n",
    "s = env.reset()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.observation_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions:  4\n",
      "Number of states:  16\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of actions: \", env.action_space.n)\n",
    "print(\"Number of states: \", env.observation_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize the Q-table \n",
    "2. Updateing of a Q value for a sate-action pair, that is Q(s,z) is given by:\n",
    "$$Q(s,a) <= Q(s,a) +a[r + gamma*maxQ(s',a')-Q(s,a)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Epsilon-Greedy approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Epsilon-Greedy is a widely used solution to the explore-exploit dilemma.\n",
    "- epsilon\n",
    "- p, ranodm probability\n",
    "- if p <= e: pull a random action\n",
    "- else: pull current best action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependency libraries\n",
    "from __future__ import print_function\n",
    "import gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state:  0\n",
      "\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Discrete(4)\n",
      "Discrete(16)\n",
      "\n",
      "number of actions:  4\n",
      "number of observations:  16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#m Load the environment\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "s = env.reset()\n",
    "print(\"Initial state: \", s)\n",
    "print()\n",
    "\n",
    "env.render()\n",
    "print()\n",
    "\n",
    "print(env.action_space) # number of actions\n",
    "print(env.observation_space) # number of states\n",
    "print()\n",
    "\n",
    "print(\"number of actions: \", env.action_space.n)\n",
    "print(\"number of observations: \", env.observation_space.n)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-Greedy approach for Exploration and Exploitation of the state-action sapces\n",
    "def epsilon_greedy(Q,s,na):\n",
    "    epsilon = 0.3\n",
    "    p = np.random.uniform(low=0, high = 1)\n",
    "    #print(p)\n",
    "    if p> epsilon:\n",
    "        return np.argmax(Q[s,:])\n",
    "    else:\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table\n",
      "[[ -9.83336029  -9.73139263  -9.36218174 -10.        ]\n",
      " [ -9.59962513  -9.53489536  -8.70996893 -10.        ]\n",
      " [ -9.83387104  -9.62517443   1.78863934 -10.        ]\n",
      " [ -9.68083191  -9.61080771  -9.69068235 -10.        ]\n",
      " [ -9.67808454  -3.06633029  -9.44978819  -9.81974568]\n",
      " [ -5.          -5.          -5.          -5.        ]\n",
      " [ -9.79996826  -9.49129635  14.57369204  -9.57911314]\n",
      " [ -5.          -5.          -5.          -5.        ]\n",
      " [ -9.61124388  14.20099495  -9.37929128  -9.78645531]\n",
      " [ -9.59511043  34.69802046  -6.23086972  -9.70972077]\n",
      " [ -9.67630806  18.31907288  41.63542884  -9.68502549]\n",
      " [ -5.          -5.          -5.          -5.        ]\n",
      " [ -5.          -5.          -5.          -5.        ]\n",
      " [ -9.65935759  58.77776549  -4.74925311  -9.19256109]\n",
      " [ -9.70748601  68.75205097 124.5491152   30.6394352 ]\n",
      " [100.         100.         100.         100.        ]]\n",
      "\n",
      "Output after leraning\n",
      "\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "===============\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "===============\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "===============\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "===============\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Initialize Q-table with zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# set hyperparameters\n",
    "lr = 0.5\n",
    "y = 0.9 # idscount factor lambda\n",
    "eps = 100000 # total number of episodes running\n",
    "\n",
    "for i in range(eps):\n",
    "    s = env.reset()\n",
    "    t = False\n",
    "    while(True):\n",
    "        a = epsilon_greedy(Q, s, env.action_space.n)\n",
    "        s_, r, t, _ = env.step(a)\n",
    "        if (r == 0):\n",
    "            if t == True:\n",
    "                r = -5 # to give negative rewards when  holes turn up\n",
    "                Q[s_] = np.ones(env.action_space.n)*r # in terminal state Q, value == reward\n",
    "            else:\n",
    "                r = -1 # to give negative rewards to avoid long routes\n",
    "        if (r ==1):\n",
    "            r = 100\n",
    "            Q[s_] = np.ones(env.action_space.n) * r \n",
    "        Q[s,a] = Q[s,a] + lr*(r+y*np.max(Q[s_,a]) - Q[s,a])\n",
    "        s = s_\n",
    "        if (t == True):\n",
    "            break\n",
    "\n",
    "print(\"Q-table\")\n",
    "print(Q)\n",
    "print()\n",
    "\n",
    "print(\"Output after leraning\")\n",
    "print()\n",
    "\n",
    "# check how much an agent has learned.\n",
    "s = env.reset()\n",
    "env.render()\n",
    "while(True):\n",
    "    a = np.argmax(Q[s])\n",
    "    s_,r,t,_ = env.step(a)\n",
    "    print(\"===============\")\n",
    "    env.render()\n",
    "    s = s_\n",
    "    if (t == True):\n",
    "        break\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
